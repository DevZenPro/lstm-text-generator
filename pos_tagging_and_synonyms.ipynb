{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download necessary files\n",
    "In Terminal, open Python and:\n",
    "1. **import nltk**\n",
    "2. **nltk.download('punkt')** for tokenizer\n",
    "3. **nltk.download('averaged_perceptron_tagger')** for pos tagger\n",
    "4. **nltk.download('wordnet')** for wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I luckily found a wonderful notebook and I probably need interesting days to write about. Hmm, can you build a country?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Hello, I luckily found a wonderful notebook and I probably need interesting days to write about. Hmm, can you build a country?'\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. POS tagging\n",
    "Reference: http://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Split text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', 'luckily', 'found', 'a', 'wonderful', 'notebook', 'and', 'I', 'probably', 'need', 'interesting', 'days', 'to', 'write', 'about', '.', 'Hmm', ',', 'can', 'you', 'build', 'a', 'country', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Lemmatize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', 'luckily', 'found', 'a', 'wonderful', 'notebook', 'and', 'I', 'probably', 'need', 'interesting', 'day', 'to', 'write', 'about', '.', 'Hmm', ',', 'can', 'you', 'build', 'a', 'country', '?']\n"
     ]
    }
   ],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "lmtzed_tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "print(lmtzed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) POS tagging with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), (',', ','), ('I', 'PRP'), ('luckily', 'RB'), ('found', 'VBD'), ('a', 'DT'), ('wonderful', 'JJ'), ('notebook', 'NN'), ('and', 'CC'), ('I', 'PRP'), ('probably', 'RB'), ('need', 'VBP'), ('interesting', 'JJ'), ('day', 'NN'), ('to', 'TO'), ('write', 'VB'), ('about', 'RB'), ('.', '.'), ('Hmm', 'NNP'), (',', ','), ('can', 'MD'), ('you', 'PRP'), ('build', 'VB'), ('a', 'DT'), ('country', 'NN'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(lmtzed_tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) POS tagging for WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(nltk_pos):\n",
    "    if nltk_pos.startswith('N'):   #Noun\n",
    "        return 'n'\n",
    "    elif nltk_pos.startswith('V'): #Verb\n",
    "        return 'v'\n",
    "    elif nltk_pos.startswith('J'): #Adjective\n",
    "        return 'a'\n",
    "    elif nltk_pos.startswith('R'): #Adverb\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemmatized token</th>\n",
       "      <th>nltk pos</th>\n",
       "      <th>wordnet pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello</td>\n",
       "      <td>Hello</td>\n",
       "      <td>NNP</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>luckily</td>\n",
       "      <td>luckily</td>\n",
       "      <td>RB</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>found</td>\n",
       "      <td>found</td>\n",
       "      <td>VBD</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>JJ</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>notebook</td>\n",
       "      <td>notebook</td>\n",
       "      <td>NN</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>probably</td>\n",
       "      <td>probably</td>\n",
       "      <td>RB</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>need</td>\n",
       "      <td>need</td>\n",
       "      <td>VBP</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>interesting</td>\n",
       "      <td>interesting</td>\n",
       "      <td>JJ</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>days</td>\n",
       "      <td>day</td>\n",
       "      <td>NN</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>write</td>\n",
       "      <td>write</td>\n",
       "      <td>VB</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>about</td>\n",
       "      <td>about</td>\n",
       "      <td>RB</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hmm</td>\n",
       "      <td>Hmm</td>\n",
       "      <td>NNP</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>can</td>\n",
       "      <td>can</td>\n",
       "      <td>MD</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "      <td>PRP</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>build</td>\n",
       "      <td>build</td>\n",
       "      <td>VB</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>country</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token lemmatized token nltk pos wordnet pos\n",
       "0         Hello            Hello      NNP           n\n",
       "1             ,                ,        ,        None\n",
       "2             I                I      PRP        None\n",
       "3       luckily          luckily       RB           r\n",
       "4         found            found      VBD           v\n",
       "5             a                a       DT        None\n",
       "6     wonderful        wonderful       JJ           a\n",
       "7      notebook         notebook       NN           n\n",
       "8           and              and       CC        None\n",
       "9             I                I      PRP        None\n",
       "10     probably         probably       RB           r\n",
       "11         need             need      VBP           v\n",
       "12  interesting      interesting       JJ           a\n",
       "13         days              day       NN           n\n",
       "14           to               to       TO        None\n",
       "15        write            write       VB           v\n",
       "16        about            about       RB           r\n",
       "17            .                .        .        None\n",
       "18          Hmm              Hmm      NNP           n\n",
       "19            ,                ,        ,        None\n",
       "20          can              can       MD        None\n",
       "21          you              you      PRP        None\n",
       "22        build            build       VB           v\n",
       "23            a                a       DT        None\n",
       "24      country          country       NN           n\n",
       "25            ?                ?        .        None"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame({'token': tokens, \n",
    "                        'lemmatized token': lmtzed_tokens, \n",
    "                        'nltk pos': [tt[1] for tt in tagged_tokens], \n",
    "                        'wordnet pos': [get_wordnet_pos(tt[1]) for tt in tagged_tokens]}, \n",
    "                       columns=['token', 'lemmatized token', 'nltk pos', 'wordnet pos'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Search synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_synonyms(word, pos):\n",
    "    if pos is None:\n",
    "        return ''\n",
    "    synsets = wn.synsets(word, pos)\n",
    "    if len(synsets) == 0:\n",
    "        return 'no synset in wordnet'\n",
    "    else:\n",
    "        synset = synsets[0]\n",
    "        synonyms = synset.lemma_names()\n",
    "        if word in synonyms:\n",
    "            synonyms.remove(word)\n",
    "        return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemmatized token</th>\n",
       "      <th>nltk pos</th>\n",
       "      <th>wordnet pos</th>\n",
       "      <th>synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello</td>\n",
       "      <td>Hello</td>\n",
       "      <td>NNP</td>\n",
       "      <td>n</td>\n",
       "      <td>[hello, hullo, hi, howdy, how-do-you-do]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>luckily</td>\n",
       "      <td>luckily</td>\n",
       "      <td>RB</td>\n",
       "      <td>r</td>\n",
       "      <td>[fortunately, fortuitously, as_luck_would_have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>found</td>\n",
       "      <td>found</td>\n",
       "      <td>VBD</td>\n",
       "      <td>v</td>\n",
       "      <td>[establish, set_up, launch]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>JJ</td>\n",
       "      <td>a</td>\n",
       "      <td>[fantastic, grand, howling, marvelous, marvell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>notebook</td>\n",
       "      <td>notebook</td>\n",
       "      <td>NN</td>\n",
       "      <td>n</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>probably</td>\n",
       "      <td>probably</td>\n",
       "      <td>RB</td>\n",
       "      <td>r</td>\n",
       "      <td>[likely, in_all_likelihood, in_all_probability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>need</td>\n",
       "      <td>need</td>\n",
       "      <td>VBP</td>\n",
       "      <td>v</td>\n",
       "      <td>[necessitate, ask, postulate, require, take, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>interesting</td>\n",
       "      <td>interesting</td>\n",
       "      <td>JJ</td>\n",
       "      <td>a</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>days</td>\n",
       "      <td>day</td>\n",
       "      <td>NN</td>\n",
       "      <td>n</td>\n",
       "      <td>[twenty-four_hours, twenty-four_hour_period, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>write</td>\n",
       "      <td>write</td>\n",
       "      <td>VB</td>\n",
       "      <td>v</td>\n",
       "      <td>[compose, pen, indite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>about</td>\n",
       "      <td>about</td>\n",
       "      <td>RB</td>\n",
       "      <td>r</td>\n",
       "      <td>[approximately, close_to, just_about, some, ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hmm</td>\n",
       "      <td>Hmm</td>\n",
       "      <td>NNP</td>\n",
       "      <td>n</td>\n",
       "      <td>no synset in wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>can</td>\n",
       "      <td>can</td>\n",
       "      <td>MD</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "      <td>PRP</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>build</td>\n",
       "      <td>build</td>\n",
       "      <td>VB</td>\n",
       "      <td>v</td>\n",
       "      <td>[construct, make]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>country</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>n</td>\n",
       "      <td>[state, nation, land, commonwealth, res_public...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token lemmatized token nltk pos wordnet pos  \\\n",
       "0         Hello            Hello      NNP           n   \n",
       "1             ,                ,        ,        None   \n",
       "2             I                I      PRP        None   \n",
       "3       luckily          luckily       RB           r   \n",
       "4         found            found      VBD           v   \n",
       "5             a                a       DT        None   \n",
       "6     wonderful        wonderful       JJ           a   \n",
       "7      notebook         notebook       NN           n   \n",
       "8           and              and       CC        None   \n",
       "9             I                I      PRP        None   \n",
       "10     probably         probably       RB           r   \n",
       "11         need             need      VBP           v   \n",
       "12  interesting      interesting       JJ           a   \n",
       "13         days              day       NN           n   \n",
       "14           to               to       TO        None   \n",
       "15        write            write       VB           v   \n",
       "16        about            about       RB           r   \n",
       "17            .                .        .        None   \n",
       "18          Hmm              Hmm      NNP           n   \n",
       "19            ,                ,        ,        None   \n",
       "20          can              can       MD        None   \n",
       "21          you              you      PRP        None   \n",
       "22        build            build       VB           v   \n",
       "23            a                a       DT        None   \n",
       "24      country          country       NN           n   \n",
       "25            ?                ?        .        None   \n",
       "\n",
       "                                             synonyms  \n",
       "0            [hello, hullo, hi, howdy, how-do-you-do]  \n",
       "1                                                      \n",
       "2                                                      \n",
       "3   [fortunately, fortuitously, as_luck_would_have...  \n",
       "4                         [establish, set_up, launch]  \n",
       "5                                                      \n",
       "6   [fantastic, grand, howling, marvelous, marvell...  \n",
       "7                                                  []  \n",
       "8                                                      \n",
       "9                                                      \n",
       "10  [likely, in_all_likelihood, in_all_probability...  \n",
       "11  [necessitate, ask, postulate, require, take, i...  \n",
       "12                                                 []  \n",
       "13  [twenty-four_hours, twenty-four_hour_period, 2...  \n",
       "14                                                     \n",
       "15                             [compose, pen, indite]  \n",
       "16  [approximately, close_to, just_about, some, ro...  \n",
       "17                                                     \n",
       "18                               no synset in wordnet  \n",
       "19                                                     \n",
       "20                                                     \n",
       "21                                                     \n",
       "22                                  [construct, make]  \n",
       "23                                                     \n",
       "24  [state, nation, land, commonwealth, res_public...  \n",
       "25                                                     "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary['synonyms'] = summary.apply(lambda row: get_synonyms(row['lemmatized token'], row['wordnet pos']), axis=1)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Replace randomly selected tokens with synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_with_wordnet_synonyms(text, n_tokens2replace):\n",
    "    \n",
    "    def wordnet_pos_tag(tokens):\n",
    "        wordnet_pos_list = []\n",
    "        for token, nltk_pos in nltk.pos_tag(tokens):\n",
    "            wordnet_pos_list.append(get_wordnet_pos(nltk_pos))\n",
    "        return wordnet_pos_list\n",
    "    \n",
    "    def get_synonyms(word, pos):\n",
    "        synsets = wn.synsets(word, pos)\n",
    "        if len(synsets) == 0:\n",
    "            return []\n",
    "        else:\n",
    "            synset = synsets[0]\n",
    "            synonyms = synset.lemma_names()\n",
    "            if word in synonyms:\n",
    "                synonyms.remove(word)\n",
    "            return synonyms\n",
    "    \n",
    "    def pick_a_random_synonym_index(n_synonyms):\n",
    "        prob = np.arange(0, n_synonyms) + 1.0\n",
    "        prob = (prob / sum(prob))[::-1]\n",
    "        return int(np.random.choice(n_synonyms, 1, p=prob))\n",
    "    \n",
    "    def swap_in_synonym(tokens, pos_list, i):\n",
    "        token = tokens[i]\n",
    "        pos = pos_list[i]\n",
    "        if pos is None:\n",
    "            return tokens\n",
    "        synonyms = get_synonyms(token, pos)\n",
    "        n_synonyms = len(synonyms)\n",
    "        if n_synonyms > 0:\n",
    "            index = pick_a_random_synonym_index(n_synonyms)\n",
    "            tokens[i] = synonyms[index]\n",
    "        return tokens\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "    pos_list = wordnet_pos_tag(tokens)\n",
    "    \n",
    "    if n_tokens2replace >= len(tokens):\n",
    "        for j in range(len(tokens)):\n",
    "            if np.random.randint(0, 2, 1) == 1:\n",
    "                swap_in_synonym(tokens, pos_list, j)\n",
    "    else:\n",
    "        n = len(tokens)\n",
    "        index_to_swap = list(set(np.random.randint(0, n, n_tokens2replace)))\n",
    "        for j in index_to_swap:\n",
    "            swap_in_synonym(tokens, pos_list, j)\n",
    "    \n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if '_' in token:\n",
    "            split_tokens_list = token.split('_')\n",
    "            split_tokens_list = [lmtzr.lemmatize(split_token) for split_token in split_tokens_list]\n",
    "            new_tokens += split_tokens_list\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of outputting 5 versions from 1 text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** original lemmatized tokens ***\n",
      "\n",
      "['Hello', ',', 'I', 'luckily', 'found', 'a', 'wonderful', 'notebook', 'and', 'I', 'probably', 'need', 'interesting', 'day', 'to', 'write', 'about', '.', 'Hmm', ',', 'can', 'you', 'build', 'a', 'country', '?']\n",
      "====================================================================================================\n",
      "*** output #1 ***\n",
      "\n",
      "['Hello', ',', 'I', 'fortunately', 'establish', 'a', 'grand', 'notebook', 'and', 'I', 'in', 'all', 'probability', 'involve', 'interesting', 'day', 'to', 'write', 'close', 'to', '.', 'Hmm', ',', 'can', 'you', 'construct', 'a', 'country', '?']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*** output #2 ***\n",
      "\n",
      "['Hello', ',', 'I', 'luckily', 'set', 'up', 'a', 'terrific', 'notebook', 'and', 'I', 'likely', 'need', 'interesting', 'day', 'to', 'write', 'just', 'about', '.', 'Hmm', ',', 'can', 'you', 'construct', 'a', 'country', '?']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*** output #3 ***\n",
      "\n",
      "['how-do-you-do', ',', 'I', 'luckily', 'found', 'a', 'wonderful', 'notebook', 'and', 'I', 'probably', 'ask', 'interesting', 'day', 'to', 'pen', 'more', 'or', 'le', '.', 'Hmm', ',', 'can', 'you', 'build', 'a', 'country', '?']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*** output #4 ***\n",
      "\n",
      "['hello', ',', 'I', 'luckily', 'found', 'a', 'wonderful', 'notebook', 'and', 'I', 'likely', 'ask', 'interesting', 'day', 'to', 'write', 'about', '.', 'Hmm', ',', 'can', 'you', 'build', 'a', 'state', '?']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "*** output #5 ***\n",
      "\n",
      "['Hello', ',', 'I', 'fortunately', 'launch', 'a', 'grand', 'notebook', 'and', 'I', 'probably', 'need', 'interesting', 'day', 'to', 'compose', 'more', 'or', 'le', '.', 'Hmm', ',', 'can', 'you', 'construct', 'a', 'country', '?']\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('*** original lemmatized tokens ***\\n')\n",
    "print(lmtzed_tokens)\n",
    "print('=' * 100)\n",
    "for i in range(5):\n",
    "    print('*** output #{} ***\\n'.format(i+1))\n",
    "    print(replace_with_wordnet_synonyms(text, 15))\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
